{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import get_arabic\n",
    "from src.settings import presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Settings(datadir=PosixPath('/home/admindme/code/MLopdracht/data/raw'), testurl=HttpUrl('https://archive.ics.uci.edu/ml/machine-learning-databases/00195/Test_Arabic_Digit.txt', ), trainurl=HttpUrl('https://archive.ics.uci.edu/ml/machine-learning-databases/00195/Train_Arabic_Digit.txt', ), testfile=PosixPath('ArabicTest.txt'), trainfile=PosixPath('ArabicTrain.txt'), modeldir=PosixPath('/home/admindme/code/MLopdracht/models'), logdir=PosixPath('/home/admindme/code/MLopdracht/logs'), modelname='model.pt', batchsize=64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 13:47:45.351 | INFO     | src.data_tools:get_file:61 - File /home/admindme/code/MLopdracht/data/raw/ArabicTrain.txt already exists, skip download\n",
      "2023-06-03 13:47:45.352 | INFO     | src.data_tools:get_file:61 - File /home/admindme/code/MLopdracht/data/raw/ArabicTest.txt already exists, skip download\n",
      "2023-06-03 13:47:45.353 | INFO     | src.datasets:get_arabic:33 - Loading data from /home/admindme/code/MLopdracht/data/raw/ArabicTrain.txt and /home/admindme/code/MLopdracht/data/raw/ArabicTest.txt\n",
      "2023-06-03 13:47:46.216 | INFO     | src.datasets:get_arabic:49 - Returning trainstreamer, teststreamer\n"
     ]
    }
   ],
   "source": [
    "trainstreamer, teststreamer = get_arabic(presets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 59, 13]), torch.Size([64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trainstreamer.stream()))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'plotly_mimetype+notebook'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "DELETE = True # to delete the tunedir at the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 40 # we run 40 experiments\n",
    "NUM_DATA = 200 # our data has 200 observations\n",
    "MAX_ITER = 15 # we run every experiment for a max of 15 epochs\n",
    "MODE = \"min\" # we want to maximize the mean. This can also be \"min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypertune import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, PosixPath('/home/admindme/code/MLopdracht/models/ray'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tune_dir = Path(\"../../models/ray/\")\n",
    "tune_dir.exists(), tune_dir.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = {}\n",
    "best_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 17:53:51,614\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Make sure the datadir exists.\n Found /home/admindme/code/MLopdracht/dev/notebooks/data/raw to be non-existing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m ray\u001b[39m.\u001b[39minit()\n\u001b[1;32m     67\u001b[0m \u001b[39m# have a look in src.settings to see how SearchSpace is created.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# If you want to search other ranges, you change this in the settings file.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m config \u001b[39m=\u001b[39m SearchSpace(\n\u001b[1;32m     70\u001b[0m     input_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     71\u001b[0m     output_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     72\u001b[0m     tune_dir\u001b[39m=\u001b[39;49mPath(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/ray\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mresolve(),\n\u001b[1;32m     73\u001b[0m     data_dir\u001b[39m=\u001b[39;49mPath(\u001b[39m\"\u001b[39;49m\u001b[39mdata/raw/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mresolve(),\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m reporter \u001b[39m=\u001b[39m CLIReporter()\n\u001b[1;32m     77\u001b[0m reporter\u001b[39m.\u001b[39madd_metric_column(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/MLopdracht/.venv/lib/python3.9/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code/MLopdracht/.venv/lib/python3.9/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code/MLopdracht/dev/notebooks/../../src/settingshypertuning.py:24\u001b[0m, in \u001b[0;36mBaseSearchSpace.check_path\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     22\u001b[0m datadir \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m datadir\u001b[39m.\u001b[39mexists():\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMake sure the datadir exists.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Found \u001b[39m\u001b[39m{\u001b[39;00mdatadir\u001b[39m}\u001b[39;00m\u001b[39m to be non-existing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Make sure the datadir exists.\n Found /home/admindme/code/MLopdracht/dev/notebooks/data/raw to be non-existing."
     ]
    }
   ],
   "source": [
    "from src import datasets\n",
    "from src.models import rnn_models, metrics, train_model\n",
    "from src.settingshypertuning import SearchSpace\n",
    "from pathlib import Path\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray import tune\n",
    "import torch\n",
    "import ray\n",
    "from typing import Dict\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from loguru import logger\n",
    "from filelock import FileLock\n",
    "\n",
    "\n",
    "def train(config: Dict, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    The train function should receive a config file, which is a Dict\n",
    "    ray will modify the values inside the config before it is passed to the train\n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    # we lock the datadir to avoid parallel instances trying to\n",
    "    # access the datadir\n",
    "    data_dir = config[\"data_dir\"]\n",
    "    with FileLock(data_dir / \".lock\"):\n",
    "        trainloader, testloader = datasets.get_arabic(\n",
    "            data_dir=data_dir, split=0.8, batchsize=64\n",
    "        )\n",
    "\n",
    "    # we set up the metric\n",
    "    accuracy = metrics.Accuracy()\n",
    "    # and create the model with the config\n",
    "    model = rnn_models.GRUmodel(config)\n",
    "\n",
    "    # and we start training.\n",
    "    # because we set tunewriter=True\n",
    "    # the trainloop wont try to report back to tensorboard,\n",
    "    # but will report back with tune.report\n",
    "    # this way, ray will know whats going on,\n",
    "    # and can start/pause/stop a loop\n",
    "    model = train_model.trainloop(\n",
    "        epochs=50,\n",
    "        model=model,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        learning_rate=1e-3,\n",
    "        loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "        metrics=[accuracy],\n",
    "        train_dataloader=trainloader,\n",
    "        test_dataloader=testloader,\n",
    "        log_dir=\".\",\n",
    "        train_steps=len(trainloader),\n",
    "        eval_steps=len(testloader),\n",
    "        patience=5,\n",
    "        factor=0.5,\n",
    "        tunewriter=[\"ray\"],\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    ray.init()\n",
    "\n",
    "    # have a look in src.settings to see how SearchSpace is created.\n",
    "    # If you want to search other ranges, you change this in the settings file.\n",
    "    config = SearchSpace(\n",
    "        input_size=3,\n",
    "        output_size=20,\n",
    "        tune_dir=Path(\"models/ray\").resolve(),\n",
    "        data_dir=Path(\"data/raw/\").resolve(),\n",
    "    )\n",
    "\n",
    "    reporter = CLIReporter()\n",
    "    reporter.add_metric_column(\"Accuracy\")\n",
    "\n",
    "    bohb_hyperband = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=50,\n",
    "        reduction_factor=3,\n",
    "        stop_last_trials=False,\n",
    "    )\n",
    "\n",
    "    bohb_search = TuneBOHB()\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train,\n",
    "        config=config.dict(),\n",
    "        metric=\"test_loss\",\n",
    "        mode=\"min\",\n",
    "        progress_reporter=reporter,\n",
    "        local_dir=config.tune_dir,\n",
    "        num_samples=50,\n",
    "        search_alg=bohb_search,\n",
    "        scheduler=bohb_hyperband,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
